{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a51ec6b6",
   "metadata": {},
   "source": [
    "# Coding project: Connect4 agent\n",
    "\n",
    "SASSON Charlotte - BERARD Paul - PUJOL Corentin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "201e647c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The assignment is to develop an agent for Connect4, as implemented in the PettingZoo library, using Reinforcement Learning methods.\n",
    "\n",
    "https://pettingzoo.farama.org/environments/classic/connect_four/\n",
    "\n",
    "Students need to form groups of 2 or 3 *or 4* students, and will deliver a zip containing the code and a short report on the methods used. Deadline: April 14th.\n",
    "\n",
    "The report should contain a description of the methods used and a description of the results. Here are a few questions that should be adressed in some way in the report\n",
    "What design choices did you make in the development of your algorithm?\n",
    "Describe the training procedure, the structure of the code.\n",
    "Describe how you chose important hyperparameters, what you understand from their impact.\n",
    "How did you assess the quality of your agent?\n",
    "Do you think the approach you implemented would fare well on more complex environments, like backgammon, chess, go, Starcraft?\n",
    "How would you improve it if given more time or computational power?\n",
    "Describe also the workflow and how you split the work in the group.\n",
    "\n",
    "\n",
    "The code should be clear and legible, variables well-named, and commented when helpful for comprehension. The final code should implement a class Player, with a method get_action that takes a state as specified in the PettingZoo environment and returns an integer between 0 and 6.\n",
    "\n",
    "Performance is not the main point: better to show understanding of the methods you used and their limits.  \n",
    "\n",
    "Ideas to get started: \n",
    "MCTS : https://sites.ualberta.ca/~szepesva/papers/CACM-MCTS.pdf\n",
    "\n",
    "Eligibility Traces:\n",
    "https://www.bkgm.com/articles/tesauro/tdl.html\n",
    "https://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/learning-chess.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f0ecd01",
   "metadata": {},
   "source": [
    "## Dependancies handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00092af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pip \n",
    "\n",
    "# !pip install pettingzoo\n",
    "# !pip install pygame\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5971313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd69e510",
   "metadata": {},
   "source": [
    "# Using the PettingZoo environment\n",
    "\n",
    "This notebook provides smalls chunks of code to get you started with the Connect4 project. You do not have to use this code in you final file, but you can if you wish to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc87362",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# The main difference with the standard gym api is the way the environment is queried. The `step` method return `None`. To get the data on the environment, use the `last` method\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "# state is a dictionary with two keys: observation and action_mask\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ")  # Observation is a numpy array with three coordinates, indicating the positions of the pieces of of player 0 and 1 on the the board\n",
    "print(state[\"observation\"][:, :, 0])  # Where the pieces of player 0 are\n",
    "print(state[\"observation\"][:, :, 1])  # Where the pieces of player 1 are\n",
    "\n",
    "print(state[\"action_mask\"])  # an array showing whether the actions are legal or nots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ") \n",
    "print(state[\"observation\"][:, :, 0])  \n",
    "print(state[\"observation\"][:, :, 1])  \n",
    "\n",
    "print(state[\"action_mask\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f5bcdf",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here are some implementations of trivial agents that you should be able to beat ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            self.rng = np.random.default_rng()\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self.name = \"Random Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        return self.random_choice_with_mask(np.arange(7), obs_mask[\"action_mask\"])\n",
    "\n",
    "    def random_choice_with_mask(self, arr, mask):\n",
    "        masked_arr = np.ma.masked_array(arr, mask=1 - mask)\n",
    "        if masked_arr.count() == 0:\n",
    "            return None\n",
    "        return self.rng.choice(masked_arr.compressed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayLeftmostLegal:\n",
    "    def __init__(self):\n",
    "        self.name = \"Left Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        for i, legal in enumerate(obs_mask[\"action_mask\"]):\n",
    "            if legal:\n",
    "                return i\n",
    "        return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4fdf7e",
   "metadata": {},
   "source": [
    "# Running a game\n",
    "\n",
    "\n",
    "The following function runs a full game between the two agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent0, agent1, display=False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    while not done:\n",
    "        for i, agent in enumerate([agent0, agent1]):\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            env.step(action)\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            done = terminated\n",
    "            if np.sum(obs[\"action_mask\"]) == 0:\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                return 0.5\n",
    "            if done:\n",
    "                if display:\n",
    "                    print(f\"Player {i}: {agent.name} won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = RandomPlayer()\n",
    "agent1 = PlayLeftmostLegal()\n",
    "\n",
    "play_game(env, agent0, agent1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([play_game(env, agent0, agent1, display=False) for _ in range(100)])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173cc5af",
   "metadata": {},
   "source": [
    "# Emulating a Gym environment\n",
    "\n",
    "If we fix the opposite policy, the game from the point of view of the agent is equivalent to a Gym environment. The following class implements this simulation. Then any algorithm that would work in a gym environment with the same observations will work here. \n",
    "\n",
    "Note that we implemented the possibility to be the first or the second player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvAgainstPolicy: \n",
    "    def __init__(self, env, policy, first_player=True):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.first_player = first_player\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward, terminated, _, _ = self.env.last()\n",
    "        if terminated: \n",
    "            self.last_step = obs, reward, True, False, {}\n",
    "        else: \n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "            obs, reward, terminated, _, _ = self.env.last()\n",
    "            self.last_step = obs, -reward, terminated, False, {}\n",
    "        return self.last_step\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        if not(self.first_player): \n",
    "            obs, _, _, _, _ = self.env.last()\n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "\n",
    "        self.last_step = self.env.last()\n",
    "        return self.last_step\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb3fab1",
   "metadata": {},
   "source": [
    "# Evaluating an agent against a fixed policy: \n",
    "\n",
    "Using the environment above, we can evaluate the agent against this fixed policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_policy(env, agent, policy, N_episodes=10, first_player=True):\n",
    "    eval_env = EnvAgainstPolicy(env, policy, first_player=first_player)\n",
    "    results = []\n",
    "    for _ in range(N_episodes):\n",
    "        done = False\n",
    "        eval_env.reset()\n",
    "        obs, _, _, _, _ = eval_env.last()\n",
    "        while not done:\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            eval_env.step(action)\n",
    "            obs, reward, done, _, _ = eval_env.last()\n",
    "        results.append(reward)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da1790b4",
   "metadata": {},
   "source": [
    "We can see that if both players play randomly, there is a small but significant advantage to the first player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38debdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=False))\n",
    "plt.show()\n",
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=True))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79580594",
   "metadata": {},
   "source": [
    "# Your turn \n",
    "\n",
    "Try to build a decent agent. Be creative! You can try any idea that you have: the grade is not about performance of the agent, but more about illustrating phenomena happening in Reinforcement Learning for turn-based games. It's okay to 'help' the agent in any way, as long as it follows the ideas of RL (i.e., as long as there is some learning involved).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f62771e",
   "metadata": {},
   "source": [
    "## Technical choices\n",
    "\n",
    "Pour entraîner notre agent aux Connect4, nous avons choisi d'implémenter un algorithme de Q-Learning.\n",
    "\n",
    "Fonctionnement de l'algorithme de Q-learning:\n",
    "\n",
    "L'algorithme de Q-learning est une méthode de renforcement qui permet à un agent d'apprendre à prendre des décisions en fonction de la valeur attendue des récompenses futures.\n",
    "\n",
    "Pour chaque état possible du jeu, l'agent maintient une table Q(s, a) des valeurs d'utilité attendues pour chaque action possible a, en supposant que l'agent suit une politique optimale.\n",
    "\n",
    "L'agent choisit une action en fonction de l'état actuel du jeu, en sélectionnant l'action qui maximise la valeur attendue des récompenses futures.\n",
    "\n",
    "L'agent met à jour la valeur de Q(s, a) en fonction de la récompense réelle qu'il reçoit pour cette action et de la valeur de Q(s+1, a+1) pour l'état  et l'action suivants, en utilisant la formule de mise à jour de Q-learning:\n",
    "\n",
    "Q(s, a) <- Q(s, a) + alpha * (r + gamma * max(Q(s+1, a+1)) - Q(s, a))\n",
    "\n",
    "Avec alpha le taux d'apprentissage, gamma le taux d'escompte (qui donne la priorité aux récompenses immédiates plutôt que futures), r est la récompense réelle pour l'action prise, et max(Q(s+1, a+1)) est la valeur maximale attendue des récompenses futures pour l'état suivant s+1 et toutes les actions possibles a+1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f47ea9c9",
   "metadata": {},
   "source": [
    "## Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd36961",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.2\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12cc472",
   "metadata": {},
   "source": [
    "## Initialisation de la table Q\n",
    "\n",
    "La table Q contiendra les valeurs d'action-état pour chaque état et chaque action possible. Dans ce cas, il y a 2^42 états possibles (2 pour chaque cellule de la grille) et 7 actions possibles (une pour chaque colonne de la grille) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((2**21, 7), dtype='uint8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e930cffa",
   "metadata": {},
   "source": [
    "## Entraînement de l'agent  dans l'environnement\n",
    "\n",
    "Entraînement de l'agent en exécutant des épisodes dans l'environnement Connect Four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Création d'une instance de l'environnement Connect Four V3 de PettingZoo. \n",
    "\n",
    "Cet environnement a deux agents, chacun jouant un pion différent.\n",
    "Le but est d'aligner quatre pions de la même couleur sur une grille de 6x7 cases.\n",
    "\"\"\"\n",
    "env = connect_four_v3.env()\n",
    "\n",
    "for i in range(EPISODES): #Un épisode correspond à une itération d'entraînement de l'agent (Il s'agit d'une partie complète)\n",
    "    obs = env.reset() #On réinitialise l'environnement pour que l'agent à chaque itération apprenne à jouer d'un état initial de la grille\n",
    "    done = False #Condition de sortie de la boucle (lorsque la partie est terminée)\n",
    "    while not done:\n",
    "        \n",
    "        \"\"\"\n",
    "        Choix d'une action basée sur la politique epsilon-greedy\n",
    "        \n",
    "        Si le nombre aléatoire généré est inférieur à epsilon :\n",
    "             - l'agent choisit une action aléatoire parmi les actions possibles\n",
    "\n",
    "        Sinon :\n",
    "            - il choisit l'action qui maximise la valeur Q de l'état actuel.\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform() < EPSILON: \n",
    "            action = np.random.choice(env.action_spaces[\"player_0\"])\n",
    "        else:\n",
    "            state = np.ravel_multi_index((obs['board'], obs['mark']), (2, 3) * (6,))\n",
    "            action = np.argmax(Q[state])\n",
    "            \n",
    "        #L'environnement effectue ensuite l'action choisie par l'agent et renvoie la nouvelle observation, la récompense, si l'épisode est terminé et des informations supplémentaires.\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        new_state = np.ravel_multi_index((new_obs['board'], new_obs['mark']), (2, 3) * (6,))\n",
    "        \n",
    "        # Update Q-table\n",
    "        Q[state, action] += ALPHA * (reward + GAMMA * np.max(Q[new_state]) - Q[state, action])\n",
    "        # Update state\n",
    "        obs = new_obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
